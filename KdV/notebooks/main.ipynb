{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQLfRQqEqxYB",
        "outputId": "662ba9f8-e540-4af4-dc4a-320424f91d42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# configs / defaults"
      ],
      "metadata": {
        "id": "Tv-HIIURq1St"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ml_collections"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vnm1cnMXq2nk",
        "outputId": "a9af590e-f4bf-4e1a-f079-30e10028f292"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ml_collections in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.17.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from ml_collections) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "import ml_collections\n",
        "\n",
        "\n",
        "def get_config():\n",
        "    config = ml_collections.ConfigDict()\n",
        "\n",
        "    config.use_wandb = True\n",
        "\n",
        "    # Weights & Biases\n",
        "    config.wandb = wandb = ml_collections.ConfigDict()\n",
        "    wandb.project = \"kdv\"\n",
        "    wandb.name = \"current_model\"\n",
        "    wandb.tags = None\n",
        "    wandb.group = None\n",
        "\n",
        "    # Simulation settings\n",
        "    config.data = data = ml_collections.ConfigDict()\n",
        "    data.nt = 140\n",
        "    data.nx = 256\n",
        "    data.L = 128\n",
        "    data.T = 140\n",
        "\n",
        "    # FNO Architecture\n",
        "    config.arch = arch = ml_collections.ConfigDict()\n",
        "    arch.modes = 16\n",
        "    arch.width = 64\n",
        "    arch.seed = 0\n",
        "\n",
        "    # Training\n",
        "    config.training = training = ml_collections.ConfigDict()\n",
        "    training.batch_size = 16\n",
        "    training.time_history = 20\n",
        "    training.time_future = 20\n",
        "    training.epochs = 30 * 2 * data.nt\n",
        "    training.seed = 1\n",
        "\n",
        "    # Optimizer\n",
        "    config.optim = optim = ml_collections.ConfigDict()\n",
        "    optim.optimizer = \"adamw\"\n",
        "    optim.learning_rate = 0.001\n",
        "    optim.b1 = 0.9\n",
        "    optim.b2 = 0.999\n",
        "    optim.eps = 1e-8\n",
        "    optim.eps_root = 0.0\n",
        "    optim.weight_decay = 0.01\n",
        "    optim.scale = 0.4\n",
        "    optim.boundaries = jnp.array([10, 1200, 2400, 3600])\n",
        "    return config\n"
      ],
      "metadata": {
        "id": "73d-DYqOq4I5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# models"
      ],
      "metadata": {
        "id": "2UVTQQreq9SB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "from typing import Callable, Dict, Tuple\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import ml_collections\n",
        "import optax\n",
        "from flax import linen as nn\n",
        "from flax.training.train_state import TrainState\n",
        "from jax import jit, random, value_and_grad, vmap\n",
        "\n",
        "\n",
        "def _create_optimizer(\n",
        "    config: ml_collections.ConfigDict,\n",
        ") -> optax.GradientTransformation:\n",
        "\n",
        "    learning_rate = config.learning_rate\n",
        "\n",
        "    keys_arr = config.boundaries\n",
        "    vals_arr = learning_rate * config.scale ** jnp.arange(\n",
        "        1, keys_arr.shape[0] + 1\n",
        "    )\n",
        "\n",
        "    def dictionary_based_schedule(step, keys_arr, vals_arr, default_lr):\n",
        "        idx = jnp.sum(keys_arr <= step) - 1\n",
        "        return jnp.where(idx < 0, default_lr, vals_arr[idx])\n",
        "\n",
        "    lr = lambda step: dictionary_based_schedule(\n",
        "        step, keys_arr, vals_arr, learning_rate\n",
        "    )\n",
        "\n",
        "    optimizer = optax.adamw(\n",
        "        learning_rate=lr,\n",
        "        b1=config.b1,\n",
        "        b2=config.b2,\n",
        "        eps=config.eps,\n",
        "        eps_root=config.eps_root,\n",
        "        weight_decay=config.weight_decay,\n",
        "    )\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def _create_train_state(\n",
        "    config: ml_collections.ConfigDict,\n",
        ") -> TrainState:\n",
        "    arch = config.arch\n",
        "    training = config.training\n",
        "\n",
        "    time_future = training.time_future\n",
        "    time_history = training.time_history\n",
        "\n",
        "    model = FNO1d(width=arch.width, modes=arch.modes, time_future=time_future)\n",
        "\n",
        "    dummy_input = jnp.ones((config.data.nx, time_history))\n",
        "    key = random.PRNGKey(arch.seed)\n",
        "    params = model.init(key, dummy_input)\n",
        "\n",
        "    tx = _create_optimizer(config.optim)\n",
        "\n",
        "    state = TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
        "    return state\n",
        "\n",
        "\n",
        "class FNO:\n",
        "    def __init__(\n",
        "        self,\n",
        "        config: ml_collections.ConfigDict,\n",
        "    ):\n",
        "        self.config = config\n",
        "        self.state = _create_train_state(config)\n",
        "\n",
        "    def loss(\n",
        "        self,\n",
        "        params: Dict,\n",
        "        state: TrainState,\n",
        "        batch: Tuple[jnp.ndarray, jnp.ndarray],\n",
        "    ) -> jnp.ndarray:\n",
        "        data, labels = batch\n",
        "        pred = vmap(lambda x: state.apply_fn(params, x))(data)\n",
        "        loss = jnp.square(pred - labels)\n",
        "        return loss.sum()\n",
        "\n",
        "    @partial(jit, static_argnums=(0,))\n",
        "    def step(\n",
        "        self,\n",
        "        state: TrainState,\n",
        "        batch: Tuple[jnp.ndarray, jnp.ndarray],\n",
        "    ) -> TrainState:\n",
        "        loss, grads = value_and_grad(self.loss)(state.params, state, batch)\n",
        "        state = state.apply_gradients(grads=grads)\n",
        "        return state, loss\n",
        "\n",
        "\n",
        "class SpectralConv1d(nn.Module):\n",
        "    width: int\n",
        "    modes: int\n",
        "\n",
        "    def setup(\n",
        "        self,\n",
        "    ):\n",
        "        scale = 1 / (self.width * self.width)\n",
        "        self.weights = self.param(\n",
        "            \"global_kernel\",\n",
        "            lambda rng, shape: random.uniform(\n",
        "                rng, shape, minval=0, maxval=scale\n",
        "            ),\n",
        "            (2, self.modes, self.width, self.width),\n",
        "        )\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(\n",
        "        self,\n",
        "        x: jnp.ndarray,\n",
        "    ) -> jnp.ndarray:\n",
        "        spatial_resolution = x.shape[0]\n",
        "\n",
        "        x_ft = jnp.fft.rfft(x, axis=0)\n",
        "        x_ft_trunc = x_ft[: self.modes, :]\n",
        "\n",
        "        R = jax.lax.complex(self.weights[0, ...], self.weights[1, ...])\n",
        "\n",
        "        R_x_ft = jnp.einsum(\"Mio,Mi->Mo\", R, x_ft_trunc)\n",
        "\n",
        "        result = jnp.zeros((x_ft.shape[0], self.width), dtype=x_ft.dtype)\n",
        "        result = result.at[: self.modes, :].set(R_x_ft)\n",
        "\n",
        "        inv_ft_R_x_ft = jnp.fft.irfft(result, n=spatial_resolution, axis=0)\n",
        "        return inv_ft_R_x_ft\n",
        "\n",
        "\n",
        "class FNOBlock1d(nn.Module):\n",
        "    width: int\n",
        "    modes: int\n",
        "    activation: Callable\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(\n",
        "        self,\n",
        "        x: jnp.ndarray,\n",
        "    ) -> jnp.ndarray:\n",
        "        spectral_conv = SpectralConv1d(self.width, self.modes)(x)\n",
        "        local_conv = nn.Conv(self.width, kernel_size=(1,), name=\"local\")(x)\n",
        "        return self.activation(spectral_conv + local_conv)\n",
        "\n",
        "\n",
        "class FNO1d(nn.Module):\n",
        "    width: int\n",
        "    modes: int\n",
        "    time_future: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(\n",
        "        self,\n",
        "        x: jnp.ndarray,\n",
        "    ) -> jnp.ndarray:\n",
        "        x = nn.Dense(self.width)(x)\n",
        "\n",
        "        x = FNOBlock1d(self.width, self.modes, nn.gelu)(x)\n",
        "        x = FNOBlock1d(self.width, self.modes, nn.gelu)(x)\n",
        "        x = FNOBlock1d(self.width, self.modes, nn.gelu)(x)\n",
        "        x = FNOBlock1d(self.width, self.modes, nn.gelu)(x)\n",
        "\n",
        "        x = nn.Dense(128)(x)\n",
        "        x = nn.gelu(x)\n",
        "        x = nn.Dense(self.time_future)(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "keOw0Lsiq99A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# utils"
      ],
      "metadata": {
        "id": "BaQ1EwE6rCIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import jax.numpy as jnp\n",
        "import torch\n",
        "import h5py\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "def to_coords(\n",
        "    x: torch.Tensor,\n",
        "    t: torch.Tensor,\n",
        ") -> torch.Tensor:\n",
        "    x_, t_ = torch.meshgrid(x, t)\n",
        "    x_ = x_.T\n",
        "    t_ = t_.T\n",
        "    return torch.stack((x_, t_), -1)\n",
        "\n",
        "\n",
        "class HDF5Dataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        path: str,\n",
        "        mode: str,\n",
        "        nt: int,\n",
        "        nx: int,\n",
        "        dtype=torch.float64,\n",
        "        load_all: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        f = h5py.File(path, \"r\")\n",
        "        self.mode = mode\n",
        "        self.dtype = dtype\n",
        "        self.data = f[self.mode]\n",
        "        self.dataset = f\"pde_{nt}-{nx}\"\n",
        "\n",
        "        if load_all:\n",
        "            data = {self.dataset: self.data[self.dataset][:]}\n",
        "            f.close()\n",
        "            self.data = data\n",
        "\n",
        "    def __len__(\n",
        "        self,\n",
        "    ) -> int:\n",
        "        return self.data[self.dataset].shape[0]\n",
        "\n",
        "    def __getitem__(\n",
        "        self,\n",
        "        idx: int,\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        u = self.data[self.dataset][idx]\n",
        "        x = self.data[\"x\"][idx]\n",
        "        t = self.data[\"t\"][idx]\n",
        "        dx = self.data[\"dx\"][idx]\n",
        "        dt = self.data[\"dt\"][idx]\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            X = to_coords(torch.tensor(x), torch.tensor(t))\n",
        "            sol = (torch.tensor(u), X)\n",
        "            u = sol[0]\n",
        "            X = sol[1]\n",
        "            dx = X[0, 1, 0] - X[0, 0, 0]\n",
        "            dt = X[1, 0, 1] - X[0, 0, 1]\n",
        "        else:\n",
        "            u = torch.from_numpy(u)\n",
        "            dx = torch.tensor([dx])\n",
        "            dt = torch.tensor([dt])\n",
        "        return u.float(), dx.float(), dt.float()\n",
        "\n",
        "\n",
        "def create_dataloader(\n",
        "    data_string: str,\n",
        "    mode: str,\n",
        "    nt: int,\n",
        "    nx: int,\n",
        "    batch_size: int,\n",
        ") -> DataLoader:\n",
        "    try:\n",
        "        dataset = HDF5Dataset(data_string, mode, nt=nt, nx=nx)\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    except:\n",
        "        raise Exception(\"Datasets could not be loaded properly\")\n",
        "    return loader\n",
        "\n",
        "\n",
        "def create_data(\n",
        "    datapoints: torch.Tensor,\n",
        "    start_time: list,\n",
        "    time_future: int,\n",
        "    time_history: int,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    data = torch.Tensor()\n",
        "    labels = torch.Tensor()\n",
        "\n",
        "    for dp, start in zip(datapoints, start_time):\n",
        "        end_time = start + time_history\n",
        "        d = dp[start:end_time]\n",
        "        target_start_time = end_time\n",
        "        target_end_time = target_start_time + time_future\n",
        "        l = dp[target_start_time:target_end_time]\n",
        "\n",
        "        data = torch.cat((data, d[None, :]), 0)\n",
        "        labels = torch.cat((labels, l[None, :]), 0)\n",
        "    return jnp.array(data), jnp.array(labels)\n"
      ],
      "metadata": {
        "id": "Ro7wsEUZrDkv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train"
      ],
      "metadata": {
        "id": "OT_etqx9rFXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "import jax.numpy as jnp\n",
        "import ml_collections\n",
        "import numpy as npy\n",
        "from flax.training.train_state import TrainState\n",
        "from torch.utils.data import DataLoader\n",
        "from wandb.sdk.wandb_run import Run\n",
        "from tqdm import trange\n",
        "from jax import random\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    config: ml_collections.ConfigDict,\n",
        "    run: Run = None,\n",
        ") -> TrainState:\n",
        "    def train(\n",
        "        state: TrainState,\n",
        "        key: jnp.ndarray,\n",
        "        loader: DataLoader,\n",
        "    ) -> Tuple[TrainState, List, jnp.ndarray]:\n",
        "        time_history = training.time_history\n",
        "        time_future = training.time_future\n",
        "        batch_size = training.batch_size\n",
        "\n",
        "        max_start_time = (config.data.nt - time_history) - time_future\n",
        "        possible_start_times = jnp.arange(\n",
        "            time_history, max_start_time + 1, time_history\n",
        "        )\n",
        "\n",
        "        losses = npy.zeros(len(loader))\n",
        "        for i, (u, _, _) in enumerate(loader):\n",
        "            key, subkey = random.split(key)\n",
        "            start_time = random.choice(\n",
        "                subkey, possible_start_times, (batch_size,)\n",
        "            )\n",
        "\n",
        "            data, labels = create_data(\n",
        "                u, start_time, time_history, time_future\n",
        "            )\n",
        "\n",
        "            batch = (\n",
        "                jnp.permute_dims(data, (0, 2, 1)),\n",
        "                jnp.permute_dims(labels, (0, 2, 1)),\n",
        "            )\n",
        "            state, loss = model.step(state, batch)\n",
        "            losses[i] = loss.item()\n",
        "        return state, losses / batch_size, key\n",
        "\n",
        "    training = config.training\n",
        "\n",
        "    train_loader = create_dataloader(\n",
        "        \"/content/drive/My Drive/data/KdV_train.h5\",\n",
        "        mode=\"train\",\n",
        "        nt=config.data.nt,\n",
        "        nx=config.data.nx,\n",
        "        batch_size=training.batch_size,\n",
        "    )\n",
        "\n",
        "    epochs = training.epochs\n",
        "    pbar = trange(epochs)\n",
        "\n",
        "    model = FNO(config)\n",
        "\n",
        "    key = random.PRNGKey(training.seed)\n",
        "    for epoch in pbar:\n",
        "        model.state, losses, key = train(model.state, key, train_loader)\n",
        "\n",
        "        if (epoch % 10 == 0) or (epoch == epochs):\n",
        "            loss = losses.mean()\n",
        "            pbar.set_postfix({\"train_loss\": loss})\n",
        "            if run is not None:\n",
        "                run.log({\"train_loss\": loss})\n",
        "    return model.state\n"
      ],
      "metadata": {
        "id": "8KnoT84FrGeR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main"
      ],
      "metadata": {
        "id": "csYHTUbErKYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "config = get_config()\n",
        "\n",
        "\n",
        "def main():\n",
        "    use_wandb = config.use_wandb\n",
        "\n",
        "    if use_wandb == True:\n",
        "        wandb.init(\n",
        "            project=config.wandb.project,\n",
        "            name=config.wandb.name,\n",
        "            tags=config.wandb.tags,\n",
        "            group=config.wandb.group,\n",
        "        )\n",
        "        run = wandb.run\n",
        "    else:\n",
        "        run = None\n",
        "    model_state = train_model(config, run)\n",
        "    return model_state, wandb.run\n",
        "\n",
        "\n",
        "model_state, run = main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "M87cOBdQrKpo",
        "outputId": "99466c1e-dd84-4899-c396-4f6549a68be5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241220_043201-67gh1zsp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/skoohy-penn/kdv/runs/67gh1zsp' target=\"_blank\">current_model</a></strong> to <a href='https://wandb.ai/skoohy-penn/kdv' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/skoohy-penn/kdv' target=\"_blank\">https://wandb.ai/skoohy-penn/kdv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/skoohy-penn/kdv/runs/67gh1zsp' target=\"_blank\">https://wandb.ai/skoohy-penn/kdv/runs/67gh1zsp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/8400 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "100%|██████████| 8400/8400 [3:37:46<00:00,  1.56s/it, train_loss=0.149]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save model"
      ],
      "metadata": {
        "id": "GobQ5uk0WgZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if run is not None:\n",
        "    import pickle\n",
        "\n",
        "    model_params_filename = f\"{run.name}_model_params\"\n",
        "    with open(model_params_filename + \".pkl\", \"wb\") as f:\n",
        "        pickle.dump(model_state.params, f)\n",
        "\n",
        "    artifact = wandb.Artifact(model_params_filename, type=\"model\")\n",
        "    artifact.add_file(model_params_filename + \".pkl\")\n",
        "    wandb.log_artifact(artifact)\n",
        "    wandb.finish()\n"
      ],
      "metadata": {
        "id": "MBzUM5q0xo-9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "03a34139-c570-47e3-fb3f-f1722341f60b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>█▆▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>0.14933</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">current_model</strong> at: <a href='https://wandb.ai/skoohy-penn/kdv/runs/67gh1zsp' target=\"_blank\">https://wandb.ai/skoohy-penn/kdv/runs/67gh1zsp</a><br> View project at: <a href='https://wandb.ai/skoohy-penn/kdv' target=\"_blank\">https://wandb.ai/skoohy-penn/kdv</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241220_043201-67gh1zsp/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}